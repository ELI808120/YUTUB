import urllib.request
import urllib.parse
import json
import time
import re
import os
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# =============================================================================
# CONFIGURATION
# =============================================================================
HISTORY_FILE = "final_history_final.json"
PENDING_FILE = "pending_check.json"
BLOCK_LOG = "block_patterns.json"

# ×”×’×‘×œ×ª ×–××Ÿ ×§×©×™×—×” (8 ×“×§×•×ª ×¨×™×¦×” × ×˜×•, ××©××™×¨ 2 ×“×§×•×ª ×œ×©××™×¨×” ×‘×’×™×˜×”×‘)
MAX_RUNTIME = 480 
MAX_WORKERS = 10  # ××¡×¤×¨ ×”×ª×”×œ×™×›×•× ×™× ×‘××§×‘×™×œ

# ××™×œ×•×ª ××¤×ª×— ×××•×§×“×•×ª ×œ×ª×•×›×Ÿ ×©×¢×•×‘×¨ ×¡×™× ×•×Ÿ (××“×¢, ×˜×‘×¢, ×œ×™××•×“×™×, ×˜×›× ×•×œ×•×’×™×”)
SAFE_SEEDS = [
    "How it's made documentary", "Python tutorial for beginners", 
    "National Geographic 4K", "Science experiments at home", 
    "Woodworking tips", "Restoration projects", "Space facts 2024",
    "History of technology", "Learn English conversation", "Physics explained"
]

class SmartFilter:
    def __init__(self, history):
        self.bad_words = ['gaming', 'stream', 'live', 'fortnite', 'minecraft', 'tiktok', 'shorts', 'music', 'official video']
        self.good_words = ['tutorial', 'guide', 'lesson', 'documentary', 'science', 'tech', 'review', 'build', 'restoration']
        self.history_ids = {item['id'] for item in history}

    def score(self, title):
        title_lower = title.lower()
        # ×¡×™× ×•×Ÿ ×’×¡ - ×× ××›×™×œ ××™×œ×” ×‘×¢×™×™×ª×™×ª, ×–×¨×•×§ ××™×“
        if any(bad in title_lower for bad in self.bad_words):
            return -100
        
        score = 0
        if any(good in title_lower for good in self.good_words):
            score += 50
        
        # ×ª××™×›×” ×‘×¢×‘×¨×™×ª (×‘×•× ×•×¡ ×’×‘×•×”)
        if any("\u0590" <= c <= "\u05EA" for c in title):
            score += 80
            
        return score

    def is_new(self, v_id):
        return v_id not in self.history_ids

class CloudCrawler:
    def __init__(self):
        self.start_time = time.time()
        self.ua = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'

    def is_time_up(self):
        return (time.time() - self.start_time) > MAX_RUNTIME

    def fetch_html(self, url):
        if self.is_time_up(): return None
        try:
            req = urllib.request.Request(url, headers={'User-Agent': self.ua})
            with urllib.request.urlopen(req, timeout=10) as response:
                return response.read().decode('utf-8', errors='ignore')
        except:
            return None

    def search_keyword(self, query):
        """××‘×¦×¢ ×—×™×¤×•×© ×•××—×–×™×¨ ×ª×•×¦××•×ª"""
        url = f"https://www.youtube.com/results?search_query={urllib.parse.quote(query)}"
        html = self.fetch_html(url)
        if not html: return []
        return self._extract_data(html)

    def get_related(self, video_id):
        """××‘×™× ×”××œ×¦×•×ª ××¡×¨×˜×•×Ÿ ×§×™×™×"""
        url = f"https://www.youtube.com/watch?v={video_id}"
        html = self.fetch_html(url)
        if not html: return []
        return self._extract_data(html)

    def _extract_data(self, html):
        """×—×™×œ×•×¥ ××”×™×¨ ×‘×××¦×¢×•×ª Regex"""
        results = []
        # ×ª×‘× ×™×ª ×©××—×œ×¦×ª ID ×•-Title
        pattern = r'"videoId":"([a-zA-Z0-9_-]{11})".*?"title":\{"runs":\[\{"text":"(.*?)"\}\]'
        matches = re.findall(pattern, html)
        for vid, title in matches:
            results.append({"id": vid, "title": title})
        return results

def main():
    print(f"ğŸš€ Crawler Started. Time Limit: {MAX_RUNTIME}s")
    
    # 1. ×˜×¢×™× ×ª × ×ª×•× ×™×
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f: history = json.load(f)
    except: history = []
    
    brain = SmartFilter(history)
    crawler = CloudCrawler()
    candidates = []
    
    # 2. ×‘× ×™×™×ª ×¨×©×™××ª ××©×™××•×ª (Tasks)
    tasks = []
    
    # ×. ×—×™×¤×•×©×™× ××‘×•×¡×¡×™ ××™×œ×•×ª ××¤×ª×—
    for seed in random.sample(SAFE_SEEDS, 5): 
        tasks.append(('search', seed))
        
    # ×‘. ×—×™×¤×•×© ×¡×‘×™×‘ ×”×¦×œ×—×•×ª ×¢×‘×¨ (×”×›×™ ×—×©×•×‘!)
    # ×œ×•×§×— 5 ×¡×¨×˜×•× ×™× ××—×¨×•× ×™× ××”×”×™×¡×˜×•×¨×™×” ×•××—×¤×© ×“×•××™× ×œ×”×
    if history:
        recent_successes = history[:5]
        for item in recent_successes:
            tasks.append(('related', item['id']))

    print(f"ğŸ“‹ Generated {len(tasks)} harvesting tasks. Executing parallel crawl...")

    # 3. ×‘×™×¦×•×¢ ×¡×¨×™×§×” ×‘××§×‘×™×œ (Multi-threading)
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        future_to_task = {}
        
        for task_type, value in tasks:
            if crawler.is_time_up(): break
            
            if task_type == 'search':
                future = executor.submit(crawler.search_keyword, value)
            else:
                future = executor.submit(crawler.get_related, value)
            future_to_task[future] = f"{task_type}:{value}"

        # ××™×¡×•×£ ×ª×•×¦××•×ª ×‘×–××Ÿ ×××ª
        for future in as_completed(future_to_task):
            task_name = future_to_task[future]
            try:
                data = future.result()
                if data:
                    print(f"   âœ… {task_name} -> Found {len(data)} videos")
                    for vid in data:
                        if brain.is_new(vid['id']):
                            score = brain.score(vid['title'])
                            if score > 0: # ×©×•××¨ ×¨×§ ×× ×”×¦×™×•×Ÿ ×—×™×•×‘×™
                                candidates.append({
                                    "id": vid['id'],
                                    "title": vid['title'],
                                    "score": score
                                })
            except Exception as e:
                print(f"   âŒ Error in {task_name}: {e}")

            if crawler.is_time_up():
                print("â³ Time limit reached! Stopping crawler...")
                break

    # 4. ×¡×™× ×•×Ÿ ×¡×•×¤×™, ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª ×•××™×•×Ÿ
    # ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª ×œ×¤×™ ID
    unique_candidates = {v['id']: v for v in candidates}.values()
    final_list = list(unique_candidates)
    
    # ××™×•×Ÿ: ×”×›×™ ××‘×˜×™×— ×œ××¢×œ×”
    final_list.sort(key=lambda x: x['score'], reverse=True)
    
    # ×—×™×ª×•×š ×œ×›××•×ª ×¡×‘×™×¨×” (×›×“×™ ×œ× ×œ×”×¢××™×¡ ×¢×œ ×”×¡×•×¨×§ ×”××§×•××™)
    final_output = final_list[:300]

    print(f"ğŸ’¾ Saving {len(final_output)} candidates to {PENDING_FILE}")
    
    with open(PENDING_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, indent=2, ensure_ascii=False)

if __name__ == "__main__":
    main()
